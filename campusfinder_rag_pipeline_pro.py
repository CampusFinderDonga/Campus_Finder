
import os
import sys
import re
from typing import List, Dict, Tuple, Optional
from dotenv import load_dotenv

# LangChain / Upstage
from langchain_upstage import UpstageEmbeddings, ChatUpstage, UpstageDocumentParseLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Retrievers & Chains
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory


# ==============================
# ðŸ”§ 0. í™˜ê²½ì„¤ì •
# ==============================
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
if not UPSTAGE_API_KEY:
    raise ValueError("âŒ Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# ë£¨íŠ¸ ë°ì´í„° ë””ë ‰í„°ë¦¬ (ì‚¬ìš©ìž í´ë” íŠ¸ë¦¬ì˜ ìµœìƒìœ„)
ROOT_DATA_DIR = os.getenv("CAMPUSFINDER_DATA_DIR", "CampusFinder_Data")

# Chroma ë²¡í„° DB ê²½ë¡œ
DB_PATH = os.getenv("CF_CHROMA_DIR", "chroma_cf_pro")

# ì²­í¬ ê¸°ë³¸ ì„¤ì •
CHUNK_SIZE = int(os.getenv("CF_CHUNK_SIZE", "700"))
CHUNK_OVERLAP = int(os.getenv("CF_CHUNK_OVERLAP", "120"))

# ê²€ìƒ‰ ê¸°ë³¸ê°’
TOP_K = int(os.getenv("CF_TOP_K", "8"))
N_QUERIES = int(os.getenv("CF_N_QUERIES", "4"))  # Multi-Query ê°œìˆ˜


# ==============================
# ðŸ“ 1. í´ë” ìˆœíšŒ: PDF ìˆ˜ì§‘
# ==============================
def iter_pdfs(root_dir: str) -> List[Tuple[str, str]]:
    """
    ë£¨íŠ¸ í´ë” í•˜ìœ„ì˜ ëª¨ë“  PDF ê²½ë¡œë¥¼ ì°¾ì•„ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜: [(absolute_pdf_path, category_path_rel), ...]
      - category_path_rel: ë£¨íŠ¸ë¡œë¶€í„°ì˜ ìƒëŒ€ í´ë” ê²½ë¡œ (ì˜ˆ: 'ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì„±ì ')
    """
    results: List[Tuple[str, str]] = []
    for dirpath, _, filenames in os.walk(root_dir):
        rel_dir = os.path.relpath(dirpath, root_dir)
        rel_dir = "" if rel_dir == "." else rel_dir.replace("\\", "/")
        for fname in filenames:
            if fname.lower().endswith(".pdf"):
                results.append((os.path.join(dirpath, fname), rel_dir))
    results.sort()
    return results


# ==============================
# ðŸ“„ 2. PDF â†’ Documents (Upstage Document Parse)
# ==============================
def parse_pdf_to_pages(pdf_path: str) -> List[str]:
    """
    UpstageDocumentParseLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDFë¥¼ íŽ˜ì´ì§€ ë‹¨ìœ„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜: íŽ˜ì´ì§€ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    loader = UpstageDocumentParseLoader(pdf_path, split="page")
    docs = loader.load()
    return [d.page_content for d in docs]


# ==============================
# âœ‚ï¸ 3. íŽ˜ì´ì§€ í…ìŠ¤íŠ¸ â†’ ì²­í¬ ë¶„í• 
# ==============================
def split_pages_to_chunks(pages: List[str]) -> List[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    chunks: List[str] = []
    for page_text in pages:
        chunks.extend(splitter.split_text(page_text))
    return chunks


# ==============================
# ðŸ§  4. ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•/ì ìž¬
# ==============================
def build_or_load_vectorstore(root_dir: str, db_path: str) -> Chroma:
    """
    ì´ë¯¸ DBê°€ ì¡´ìž¬í•˜ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
    ê° ì²­í¬ì—ëŠ” ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ìž¥í•©ë‹ˆë‹¤.
      - source: íŒŒì¼ëª… (í™•ìž¥ìž ì œì™¸)
      - category: ë£¨íŠ¸ë¡œë¶€í„°ì˜ í´ë” ê²½ë¡œ (ì˜ˆ: 'ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì„±ì ')
      - file_path: ë£¨íŠ¸ë¡œë¶€í„°ì˜ íŒŒì¼ ìƒëŒ€ê²½ë¡œ (ì¹´í…Œê³ ë¦¬/íŒŒì¼.pdf)
    """
    # ì´ë¯¸ DBê°€ ìžˆìœ¼ë©´ ë¡œë“œ
    if os.path.isdir(db_path) and len(os.listdir(db_path)) > 0:
        embedding = UpstageEmbeddings(model="solar-embedding-1-large")
        return Chroma(persist_directory=db_path, embedding_function=embedding)

    print("ðŸ“š ì‹ ê·œ ìž„ë² ë”© ì‹œìž‘â€¦ (ì²˜ìŒ 1íšŒë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤)\n")
    pdfs = iter_pdfs(root_dir)
    if not pdfs:
        raise FileNotFoundError(f"âŒ '{root_dir}' ì—ì„œ PDFë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í´ë” ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    all_texts: List[str] = []
    all_metadatas: List[Dict[str, str]] = []

    for idx, (abs_pdf, rel_category) in enumerate(pdfs, 1):
        rel_file_path = os.path.relpath(abs_pdf, root_dir).replace("\\", "/")
        file_stem = os.path.splitext(os.path.basename(abs_pdf))[0]

        print(f"[{idx}/{len(pdfs)}] ðŸ” íŒŒì‹±: {rel_file_path}")
        try:
            pages = parse_pdf_to_pages(abs_pdf)
            chunks = split_pages_to_chunks(pages)
            # ë©”íƒ€ë°ì´í„° ë¶€ì°©
            for ch in chunks:
                all_texts.append(ch)
                all_metadatas.append({
                    "source": file_stem,
                    "category": rel_category,
                    "file_path": rel_file_path,
                })
        except Exception as e:
            print(f"   âš ï¸ íŒŒì‹± ì‹¤íŒ¨: {rel_file_path} -> {e}")
            continue

    print(f"\nðŸ§  ë²¡í„° ìž„ë² ë”© ë³€í™˜ ì¤‘â€¦ (ì´ {len(all_texts)}ê°œ ì²­í¬)")
    embedding = UpstageEmbeddings(model="solar-embedding-1-large")
    vectorstore = Chroma.from_texts(
        texts=all_texts,
        embedding=embedding,
        metadatas=all_metadatas,
        persist_directory=db_path,
    )
    print("âœ… ë²¡í„° ì €ìž¥ ì™„ë£Œ!")
    return vectorstore


# ==============================
# ðŸ§­ 5. ì¹´í…Œê³ ë¦¬ ë£° & ë§¤í•‘
# ==============================
CATEGORY_RULES: List[Tuple[str, str]] = [
    # í•™ì‚¬ì•ˆë‚´ í•˜ìœ„
    ("ìˆ˜ì—…",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ìˆ˜ì—…"),
    ("êµê³¼ê³¼ì •",           "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/êµê³¼ê³¼ì •"),
    ("í•™ì ",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/í•™ì "),
    ("ì„±ì ",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì„±ì "),
    ("ì¡¸ì—…",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì¡¸ì—…"),
    ("ë‹¤ì „ê³µ",             "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë‹¤ì „ê³µì•ˆë‚´"),
    ("ë³µìˆ˜ì „ê³µ",           "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë‹¤ì „ê³µì•ˆë‚´"),
    ("ë¶€ì „ê³µ",             "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë‹¤ì „ê³µì•ˆë‚´"),
    ("í•™ì ì¸ì •",           "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/í•™ì ì¸ì •_ì‹ ì²­"),
    ("ìž¥í•™",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ìž¥í•™"),
    ("ë“±ë¡ê¸ˆ",             "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë“±ë¡"),
    ("ë“±ë¡",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë“±ë¡"),
    ("ì˜ˆë¹„êµ°",             "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì˜ˆë¹„êµ°"),
    ("í•™ì‚¬ì¼ì •",           "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì¼ì •_ë™ì•„ê´‘ìž¥"),

    # ë™ì•„ê´‘ìž¥
    ("FAQ",               "ëŒ€í•™_ê³µí†µì •ë³´/ë™ì•„ê´‘ìž¥/FAQ"),
    ("ìƒí™œì •ë³´",           "ëŒ€í•™_ê³µí†µì •ë³´/ë™ì•„ê´‘ìž¥/ìƒí™œì •ë³´"),
    ("í•™ìƒìžì¹˜",           "ëŒ€í•™_ê³µí†µì •ë³´/ë™ì•„ê´‘ìž¥/í•™ìƒìžì¹˜í™œë™"),

    # ê²½ì˜ëŒ€í•™ í•˜ìœ„
    ("ê²½ì˜í•™ê³¼",           "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê²½ì˜í•™ê³¼"),
    ("ê´€ê´‘ê²½ì˜í•™ê³¼",       "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê´€ê´‘ê²½ì˜í•™ê³¼"),
    ("êµ­ì œë¬´ì—­í•™ê³¼",       "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/êµ­ì œë¬´ì—­í•™ê³¼"),
    ("ê²½ì˜ì •ë³´í•™ê³¼",       "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê²½ì˜ì •ë³´í•™ê³¼"),
    ("ê¸ˆìœµí•™ê³¼",           "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê¸ˆìœµí•™ê³¼"),

    # ë¹„êµê³¼ & ì·¨ì—…
    ("ë¹„êµê³¼",             "ë¹„êµê³¼_DECOì‹œìŠ¤í…œ"),
    ("DECO",              "ë¹„êµê³¼_DECOì‹œìŠ¤í…œ"),
    ("ì·¨ì—…",               "ì·¨ì—…ì§€ì›_ì±„ìš©ì •ë³´"),
    ("ì±„ìš©",               "ì·¨ì—…ì§€ì›_ì±„ìš©ì •ë³´"),
]

DEPT_NAME_TO_CATEGORY = {
    "ê²½ì˜í•™ê³¼": "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê²½ì˜í•™ê³¼",
    "ê´€ê´‘ê²½ì˜í•™ê³¼": "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê´€ê´‘ê²½ì˜í•™ê³¼",
    "êµ­ì œë¬´ì—­í•™ê³¼": "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/êµ­ì œë¬´ì—­í•™ê³¼",
    "ê²½ì˜ì •ë³´í•™ê³¼": "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê²½ì˜ì •ë³´í•™ê³¼",
    "ê¸ˆìœµí•™ê³¼": "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê¸ˆìœµí•™ê³¼",
}

def map_query_to_categories(query: str) -> List[str]:
    """
    ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì§ˆì˜ì—ì„œ ì¹´í…Œê³ ë¦¬ í›„ë³´ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.
    - 'í´ë”:ì¹´í…Œê³ ë¦¬/í•˜ìœ„' í˜•íƒœê°€ ìžˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì‚¬ìš©ìž ê°•ì œ ì§€ì •)
    - ê·œì¹™ê³¼ ë§¤ì¹­ë˜ëŠ” í•­ëª©ì„ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
    """
    query = (query or "").strip()
    cats: List[str] = []

    # ì‚¬ìš©ìžê°€ ì§ì ‘ ì§€ì •: ì˜ˆ) í´ë”: í•™ì‚¬ì•ˆë‚´/ì„±ì   ë˜ëŠ”  folder: ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê²½ì˜í•™ê³¼
    for token in ["í´ë”:", "folder:"]:
        if token in query:
            after = query.split(token, 1)[1].strip()
            after = after.split()[0].strip().strip(" ,.;)ã€ã€]")
            if not after.startswith(("ëŒ€í•™_ê³µí†µì •ë³´", "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´", "ì·¨ì—…ì§€ì›_ì±„ìš©ì •ë³´", "ë¹„êµê³¼_DECOì‹œìŠ¤í…œ")):
                after = f"ëŒ€í•™_ê³µí†µì •ë³´/{after}".replace("//", "/")
            cats.append(after.replace("\\", "/"))

    q = query.replace(" ", "")
    for key, cat in CATEGORY_RULES:
        if key in q:
            cats.append(cat)

    # ì¤‘ë³µ ì œê±° & ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
    uniq = []
    for c in cats:
        if c not in uniq:
            uniq.append(c)
    return uniq[:3]


# ==============================
# ðŸ” 6. ê²€ìƒ‰ ìœ í‹¸ (ì¹´í…Œê³ ë¦¬ í•„í„° + ë©€í‹°ì¿¼ë¦¬)
# ==============================
def build_vector_retriever(vs: Chroma, top_k: int = TOP_K, categories: Optional[List[str]] = None):
    search_kwargs = {"k": top_k}
    if categories:
        if len(categories) == 1:
            search_kwargs["filter"] = {"category": categories[0]}
        else:
            search_kwargs["filter"] = {"category": {"$in": categories}}
    return vs.as_retriever(search_kwargs=search_kwargs)


def multi_query_search(
    vs: Chroma,
    question: str,
    categories: Optional[List[str]] = None,
    n_queries: int = N_QUERIES,
    top_k: int = TOP_K,
) -> List:
    """
    LLMì´ ë‹¤ì–‘í•œ í‘œí˜„ì˜ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì—¬ ê²€ìƒ‰(ë¦¬ì½œ í–¥ìƒ).
    """
    llm = ChatUpstage(model="solar-pro")
    base_retriever = build_vector_retriever(vs, top_k=top_k, categories=categories)
    mqr = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm, include_original=True)
    # MultiQueryRetriever ìžì²´ê°€ retriever ì¸í„°íŽ˜ì´ìŠ¤ë¥¼ êµ¬í˜„ (get_relevant_documents)
    docs = mqr.get_relevant_documents(question, n_queries=n_queries)
    return docs


# ==============================
# ðŸ”Ž 7. ë¹„êµí˜• ì§ˆë¬¸ ê°ì§€ & ì—”í‹°í‹° ì¶”ì¶œ
# ==============================
COMPARE_HINTS = ["ë¹„êµ", "ì°¨ì´", "ì°¨ì´ì ", "vs", "VS", "ë¹„ê²¬", "ëŒ€ì¡°", "êµ¬ë¶„", "ëŒ€ë¹„", "ì–´ë–¤ ì°¨ì´"]

def is_compare_question(q: str) -> bool:
    q = (q or "").strip()
    return any(h in q for h in COMPARE_HINTS) or ("ì™€" in q and "ì°¨ì´" in q)

def extract_departments(q: str) -> List[str]:
    found = []
    for name in DEPT_NAME_TO_CATEGORY.keys():
        if name in q:
            found.append(name)
    # ì¤‘ë³µ ì œê±°
    return list(dict.fromkeys(found))


# ==============================
# ðŸ§µ 8. ì¼ë°˜ ì§ˆì˜ìš© ëŒ€í™”í˜• ì²´ì¸
# ==============================
def build_conversation_chain(vs: Chroma, categories: Optional[List[str]] = None):
    retriever = build_vector_retriever(vs, TOP_K, categories)
    llm = ChatUpstage(model="solar-pro")

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "ë‹¹ì‹ ì€ ë™ì•„ëŒ€í•™êµ ê´€ë ¨ ê³µì‹ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” RAG ì±—ë´‡ìž…ë‹ˆë‹¤.\n"
            "ì•„ëž˜ì˜ ì°¸ê³  ë¬¸ì„œ ì¡°ê°ì„ ê·¼ê±°ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n\n"
            "ì°¸ê³  ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
            "ì§ˆë¬¸: {question}\n\n"
            "ì‘ë‹µ ê·œì¹™:\n"
            "- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
            "- í•µì‹¬ë§Œ ì •ë¦¬í•˜ì—¬ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
            "- í•„ìš”í•˜ë©´ í•­ëª©(â€¢)ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”."
        ),
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={
            "prompt": prompt,
            "document_variable_name": "context",
            "output_key": "answer",
        },
        output_key="answer",
    )
    return chain


# ==============================
# ðŸ§¾ 9. ë¹„êµí˜• ì§ˆì˜ ì²˜ë¦¬
# ==============================
def compare_two_entities(
    vs: Chroma,
    question: str,
    left_dept: str,
    right_dept: str,
    top_k: int = TOP_K,
    n_queries: int = N_QUERIES,
) -> Tuple[str, List[Tuple[str, str]]]:
    """
    ë‘ í•™ê³¼(ë˜ëŠ” ì—”í‹°í‹°)ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    - ê° ì—”í‹°í‹° ì¹´í…Œê³ ë¦¬ë¡œ í•„í„° â†’ MultiQueryë¡œ ê²€ìƒ‰ â†’ ìƒìœ„ ë¬¸ì„œë“¤ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± â†’ ë¹„êµ í”„ë¡¬í”„íŠ¸ë¡œ LLM í˜¸ì¶œ
    ë°˜í™˜: (answer_text, reference_list[(file_path, category)])
    """
    llm = ChatUpstage(model="solar-pro")

    left_cat = DEPT_NAME_TO_CATEGORY.get(left_dept)
    right_cat = DEPT_NAME_TO_CATEGORY.get(right_dept)

    left_docs = multi_query_search(vs, f"{left_dept} ì¡¸ì—… í›„ ì§„ë¡œ", categories=[left_cat], n_queries=n_queries, top_k=top_k)
    right_docs = multi_query_search(vs, f"{right_dept} ì¡¸ì—… í›„ ì§„ë¡œ", categories=[right_cat], n_queries=n_queries, top_k=top_k)

    # ë¬¸ì„œ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    def build_context(docs: List) -> str:
        # ì¤‘ë³µ ì¤„ì´ê¸° & ê¸¸ì´ ì œí•œ
        seen = set()
        chunks = []
        for d in docs[:max(top_k, 8)]:
            txt = (d.page_content or "").strip()
            if not txt or txt in seen:
                continue
            seen.add(txt)
            chunks.append(txt)
            if sum(len(c) for c in chunks) > 8000:
                break
        return "\n---\n".join(chunks)

    left_ctx = build_context(left_docs)
    right_ctx = build_context(right_docs)

    compare_prompt = PromptTemplate(
        input_variables=["question", "left", "right", "left_ctx", "right_ctx"],
        template=(
            "ë‹¹ì‹ ì€ ë™ì•„ëŒ€í•™êµ ê³µì‹ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‘ í•­ëª©ì„ ë¹„êµí•˜ëŠ” ë¶„ì„ê°€ìž…ë‹ˆë‹¤.\n"
            "ì•„ëž˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´, ì§ˆë¬¸ì— ëŒ€í•´ ë‘ í•™ê³¼ë¥¼ ë¹„êµí•´ ì£¼ì„¸ìš”.\n\n"
            "ì§ˆë¬¸: {question}\n\n"
            "ã€ì¢Œì¸¡ ëŒ€ìƒã€‘ {left}\n"
            "ì»¨í…ìŠ¤íŠ¸(L):\n{left_ctx}\n\n"
            "ã€ìš°ì¸¡ ëŒ€ìƒã€‘ {right}\n"
            "ì»¨í…ìŠ¤íŠ¸(R):\n{right_ctx}\n\n"
            "ì‘ë‹µ í˜•ì‹ ì§€ì¹¨:\n"
            "1) í•œ ë¬¸ë‹¨ ìš”ì•½(ì°¨ì´ í•µì‹¬ 2~3ì¤„)\n"
            "2) í•­ëª©ë³„ ë¹„êµ(â€¢ ì‚¬ìš©) - ì˜ˆ: ì§„ì¶œ ë¶„ì•¼, ëŒ€í‘œ ì§ë¬´, ê´€ë ¨ ì‚°ì—…/ê¸°ê´€, í•„ìš” ì—­ëŸ‰/ìžê²© ë“±\n"
            "3) ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì“°ì§€ ë§ê³  'ë¬¸ì„œì— ëª…ì‹œ ì—†ìŒ'ì´ë¼ê³  í‘œê¸°\n"
            "4) ë§ˆì§€ë§‰ì— 'ì í•©í•œ í•™ìƒ í”„ë¡œí•„'ì„ ê°„ë‹¨ížˆ ì •ë¦¬\n"
            "í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš”."
        ),
    )

    prompt_text = compare_prompt.format(
        question=question,
        left=left_dept,
        right=right_dept,
        left_ctx=left_ctx,
        right_ctx=right_ctx,
    )
    answer = llm.invoke(prompt_text).content

    # ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    refs = []
    for d in (left_docs + right_docs):
        meta = d.metadata or {}
        ref = (meta.get("file_path") or meta.get("source") or "unknown", meta.get("category", ""))
        if ref not in refs:
            refs.append(ref)

    return answer, refs


# ==============================
# ðŸš€ 10. ë©”ì¸ ë£¨í”„
# ==============================
def main():
    print("ðŸ“š CampusFinder RAG (Pro) ì´ˆê¸°í™” ì¤‘â€¦\n")
    vectorstore = build_or_load_vectorstore(ROOT_DATA_DIR, DB_PATH)
    print("\nâœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: exit/quit/ì¢…ë£Œ)\n")

    print("ðŸ’¡ ížŒíŠ¸) íŠ¹ì • í´ë”ë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ ì§ˆì˜ì— 'í´ë”: í•™ì‚¬ì•ˆë‚´/ì„±ì ' ì²˜ëŸ¼ ì ì–´ì£¼ì„¸ìš”.")
    print("   ì˜ˆ) 'ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜ í´ë”: í•™ì‚¬ì•ˆë‚´/ì¡¸ì—…'")
    print("ðŸ’¡ ë¹„êµí˜• ì§ˆë¬¸ ì˜ˆ) 'ê²½ì˜í•™ê³¼ì™€ ê´€ê´‘ê²½ì˜í•™ê³¼ ì¡¸ì—… í›„ ì§„ë¡œ ì°¨ì´'")

    qa_chain = build_conversation_chain(vectorstore)  # ê¸°ë³¸ ì²´ì¸

    while True:
        try:
            query = input("\nâ“ ì§ˆë¬¸: ").strip()
        except EOFError:
            break

        if query.lower() in {"exit", "quit", "ì¢…ë£Œ"}:
            print("ðŸ‘‹ í”„ë¡œê·¸ëž¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ë¹„êµí˜• ì§ˆë¬¸ì¸ì§€ ê°ì§€
        if is_compare_question(query):
            depts = extract_departments(query)
            # ë¬¸ìž¥ ë‚´ ì—”í‹°í‹°ê°€ 2ê°œ ì´ìƒì´ë©´ ë¹„êµ ëª¨ë“œ
            if len(depts) >= 2:
                left, right = depts[0], depts[1]
                print(f"ðŸ”Ž ë¹„êµí˜• ê²€ìƒ‰ ì ìš©: [{left}] vs [{right}]")
                try:
                    answer, refs = compare_two_entities(vectorstore, query, left, right, TOP_K, N_QUERIES)
                    print(f"\nðŸ¤– ë¹„êµ ì‘ë‹µ:\n{answer}\n")
                    if refs:
                        print("ðŸ“Ž ì°¸ê³  ë¬¸ì„œ:")
                        for fp, cat in refs:
                            cat_disp = f" ({cat})" if cat else ""
                            print(f" - {fp}{cat_disp}")
                    continue
                except Exception as e:
                    print(f"âš ï¸ ë¹„êµ ëª¨ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e} â†’ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")

        # ë¹„êµí˜•ì´ ì•„ë‹ˆê±°ë‚˜, ë¹„êµ ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ â†’ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ + ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰ í›„ ì¼ë°˜ ë‹µë³€
        cats = map_query_to_categories(query)
        if cats:
            print(f"ðŸ”Ž ì¹´í…Œê³ ë¦¬ ìš°ì„  ê²€ìƒ‰ ì ìš©: {cats}")
        try:
            # MultiQueryë¡œ ìš°ì„  ë¬¸ì„œ ìˆ˜ì§‘
            docs = multi_query_search(vectorstore, query, categories=cats or None, n_queries=N_QUERIES, top_k=TOP_K)

            # ê°„ë‹¨í•œ ìˆ˜ë™ RAG (ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± í›„ LLM í˜¸ì¶œ)
            llm = ChatUpstage(model="solar-pro")
            def build_context(docs: List) -> str:
                seen = set()
                chunks = []
                for d in docs[:max(TOP_K, 8)]:
                    txt = (d.page_content or "").strip()
                    if not txt or txt in seen:
                        continue
                    seen.add(txt)
                    chunks.append(txt)
                    if sum(len(c) for c in chunks) > 8000:
                        break
                return "\n---\n".join(chunks)

            context = build_context(docs)
            prompt = PromptTemplate(
                input_variables=["context", "question"],
                template=(
                    "ë‹¹ì‹ ì€ ë™ì•„ëŒ€í•™êµ ê³µì‹ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” RAG ë¹„ì„œìž…ë‹ˆë‹¤.\n"
                    "ì•„ëž˜ ì°¸ê³  ë‚´ìš©ì„ ê·¼ê±°ë¡œ, ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                    "ì°¸ê³  ë‚´ìš©:\n{context}\n\n"
                    "ì§ˆë¬¸: {question}\n\n"
                    "ê·œì¹™:\n"
                    "- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
                    "- í•„ìš”í•œ ê²½ìš° ë¶ˆë¦¿(â€¢)ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”."
                ),
            )
            prompt_text = prompt.format(context=context, question=query)
            answer = llm.invoke(prompt_text).content
            print(f"\nðŸ¤– ë‹µë³€:\n{answer}\n")

            # ì°¸ê³  ë¬¸ì„œ
            if docs:
                refs = []
                seen_ref = set()
                for d in docs:
                    meta = d.metadata or {}
                    ref = (meta.get("file_path") or meta.get("source") or "unknown", meta.get("category", ""))
                    if ref not in seen_ref:
                        seen_ref.add(ref)
                        refs.append(ref)
                if refs:
                    print("ðŸ“Ž ì°¸ê³  ë¬¸ì„œ:")
                    for fp, cat in refs[:12]:
                        cat_disp = f" ({cat})" if cat else ""
                        print(f" - {fp}{cat_disp}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    # ì˜µì…˜: ë£¨íŠ¸ í´ë”/DB ê²½ë¡œë¥¼ ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì „ë‹¬ ê°€ëŠ¥
    # ì‚¬ìš©: python campusfinder_rag_pipeline_pro.py [DATA_DIR] [DB_PATH]
    if len(sys.argv) >= 2:
        ROOT_DATA_DIR = sys.argv[1]
    if len(sys.argv) >= 3:
        DB_PATH = sys.argv[2]
    main()
