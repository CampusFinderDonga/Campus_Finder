
import os
import sys
from typing import List, Dict, Tuple, Optional
from dotenv import load_dotenv

# LangChain / Upstage
from langchain_upstage import UpstageEmbeddings, ChatUpstage, UpstageDocumentParseLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate


# ==============================
# ðŸ”§ 0. í™˜ê²½ì„¤ì •
# ==============================
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
if not UPSTAGE_API_KEY:
    raise ValueError("âŒ Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# ë£¨íŠ¸ ë°ì´í„° ë””ë ‰í„°ë¦¬ (ì‚¬ìš©ìž í´ë” íŠ¸ë¦¬ì˜ ìµœìƒìœ„)
ROOT_DATA_DIR = os.getenv("CAMPUSFINDER_DATA_DIR", "CampusFinder_Data")

# Chroma ë²¡í„° DB ê²½ë¡œ
DB_PATH = os.getenv("CF_CHROMA_DIR", "chroma_cf")

# ì²­í¬ ê¸°ë³¸ ì„¤ì •
CHUNK_SIZE = int(os.getenv("CF_CHUNK_SIZE", "700"))
CHUNK_OVERLAP = int(os.getenv("CF_CHUNK_OVERLAP", "120"))

# ê²€ìƒ‰ ê¸°ë³¸ê°’
TOP_K = int(os.getenv("CF_TOP_K", "8"))


# ==============================
# ðŸ“ 1. í´ë” ìˆœíšŒ: PDF ìˆ˜ì§‘
# ==============================
def iter_pdfs(root_dir: str) -> List[Tuple[str, str]]:
    """
    ë£¨íŠ¸ í´ë” í•˜ìœ„ì˜ ëª¨ë“  PDF ê²½ë¡œë¥¼ ì°¾ì•„ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜: [(absolute_pdf_path, category_path_rel), ...]
      - category_path_rel: ë£¨íŠ¸ë¡œë¶€í„°ì˜ ìƒëŒ€ í´ë” ê²½ë¡œ (ì˜ˆ: 'ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì„±ì ')
    """
    results: List[Tuple[str, str]] = []
    for dirpath, _, filenames in os.walk(root_dir):
        rel_dir = os.path.relpath(dirpath, root_dir)
        rel_dir = "" if rel_dir == "." else rel_dir.replace("\\", "/")
        for fname in filenames:
            if fname.lower().endswith(".pdf"):
                results.append((os.path.join(dirpath, fname), rel_dir))
    results.sort()
    return results


# ==============================
# ðŸ“„ 2. PDF â†’ Documents (Upstage Document Parse)
# ==============================
def parse_pdf_to_documents(pdf_path: str) -> List[str]:
    """
    UpstageDocumentParseLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDFë¥¼ íŽ˜ì´ì§€ ë‹¨ìœ„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜: íŽ˜ì´ì§€ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    loader = UpstageDocumentParseLoader(pdf_path, split="page")
    docs = loader.load()
    return [d.page_content for d in docs]


# ==============================
# âœ‚ï¸ 3. íŽ˜ì´ì§€ í…ìŠ¤íŠ¸ â†’ ì²­í¬ ë¶„í• 
# ==============================
def split_pages_to_chunks(pages: List[str]) -> List[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    chunks: List[str] = []
    for page_text in pages:
        chunks.extend(splitter.split_text(page_text))
    return chunks


# ==============================
# ðŸ§  4. ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•/ì ìž¬
# ==============================
def build_or_load_vectorstore(root_dir: str, db_path: str) -> Chroma:
    """
    ì´ë¯¸ DBê°€ ì¡´ìž¬í•˜ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
    ê° ì²­í¬ì—ëŠ” ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ìž¥í•©ë‹ˆë‹¤.
      - source: íŒŒì¼ëª… (í™•ìž¥ìž ì œì™¸)
      - category: ë£¨íŠ¸ë¡œë¶€í„°ì˜ í´ë” ê²½ë¡œ (ì˜ˆ: 'ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì„±ì ')
      - file_path: ë£¨íŠ¸ë¡œë¶€í„°ì˜ íŒŒì¼ ìƒëŒ€ê²½ë¡œ (ì¹´í…Œê³ ë¦¬/íŒŒì¼.pdf)
    """
    # ì´ë¯¸ DBê°€ ìžˆìœ¼ë©´ ë¡œë“œ
    if os.path.isdir(db_path) and len(os.listdir(db_path)) > 0:
        embedding = UpstageEmbeddings(model="solar-embedding-1-large")
        return Chroma(persist_directory=db_path, embedding_function=embedding)

    print("ðŸ“š ì‹ ê·œ ìž„ë² ë”© ì‹œìž‘â€¦ (ì²˜ìŒ 1íšŒë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤)\n")
    pdfs = iter_pdfs(root_dir)
    if not pdfs:
        raise FileNotFoundError(f"âŒ '{root_dir}' ì—ì„œ PDFë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í´ë” ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    all_texts: List[str] = []
    all_metadatas: List[Dict[str, str]] = []

    for idx, (abs_pdf, rel_category) in enumerate(pdfs, 1):
        rel_file_path = os.path.relpath(abs_pdf, root_dir).replace("\\", "/")
        file_stem = os.path.splitext(os.path.basename(abs_pdf))[0]

        print(f"[{idx}/{len(pdfs)}] ðŸ” íŒŒì‹±: {rel_file_path}")
        try:
            pages = parse_pdf_to_documents(abs_pdf)
            chunks = split_pages_to_chunks(pages)
            # ë©”íƒ€ë°ì´í„° ë¶€ì°©
            for ch in chunks:
                all_texts.append(ch)
                all_metadatas.append({
                    "source": file_stem,
                    "category": rel_category,
                    "file_path": rel_file_path,
                })
        except Exception as e:
            print(f"   âš ï¸ íŒŒì‹± ì‹¤íŒ¨: {rel_file_path} -> {e}")
            continue

    print(f"\nðŸ§  ë²¡í„° ìž„ë² ë”© ë³€í™˜ ì¤‘â€¦ (ì´ {len(all_texts)}ê°œ ì²­í¬)")
    embedding = UpstageEmbeddings(model="solar-embedding-1-large")
    vectorstore = Chroma.from_texts(
        texts=all_texts,
        embedding=embedding,
        metadatas=all_metadatas,
        persist_directory=db_path,
    )
    print("âœ… ë²¡í„° ì €ìž¥ ì™„ë£Œ!")
    return vectorstore


# ==============================
# ðŸ§­ 5. ì§ˆì˜ â†’ ì¹´í…Œê³ ë¦¬ í›„ë³´ ìžë™ ë§¤í•‘ (ë£° ê¸°ë°˜ 1ì°¨ ë²„ì „)
# ==============================
CATEGORY_RULES: List[Tuple[str, str]] = [
    # í•™ì‚¬ì•ˆë‚´ í•˜ìœ„
    ("ìˆ˜ì—…",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ìˆ˜ì—…"),
    ("êµê³¼ê³¼ì •",           "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/êµê³¼ê³¼ì •"),
    ("í•™ì ",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/í•™ì "),
    ("ì„±ì ",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì„±ì "),
    ("ì¡¸ì—…",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì¡¸ì—…"),
    ("ë‹¤ì „ê³µ",             "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë‹¤ì „ê³µì•ˆë‚´"),
    ("ë³µìˆ˜ì „ê³µ",           "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë‹¤ì „ê³µì•ˆë‚´"),
    ("ë¶€ì „ê³µ",             "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë‹¤ì „ê³µì•ˆë‚´"),
    ("í•™ì ì¸ì •",           "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/í•™ì ì¸ì •_ì‹ ì²­"),
    ("ìž¥í•™",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ìž¥í•™"),
    ("ë“±ë¡ê¸ˆ",             "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë“±ë¡"),
    ("ë“±ë¡",               "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ë“±ë¡"),
    ("ì˜ˆë¹„êµ°",             "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì•ˆë‚´/ì˜ˆë¹„êµ°"),
    ("í•™ì‚¬ì¼ì •",           "ëŒ€í•™_ê³µí†µì •ë³´/í•™ì‚¬ì¼ì •_ë™ì•„ê´‘ìž¥"),

    # ë™ì•„ê´‘ìž¥
    ("FAQ",               "ëŒ€í•™_ê³µí†µì •ë³´/ë™ì•„ê´‘ìž¥/FAQ"),
    ("ìƒí™œì •ë³´",           "ëŒ€í•™_ê³µí†µì •ë³´/ë™ì•„ê´‘ìž¥/ìƒí™œì •ë³´"),
    ("í•™ìƒìžì¹˜",           "ëŒ€í•™_ê³µí†µì •ë³´/ë™ì•„ê´‘ìž¥/í•™ìƒìžì¹˜í™œë™"),

    # ê²½ì˜ëŒ€í•™ í•˜ìœ„ (í•„ìš”ì‹œ í™•ìž¥)
    ("ê²½ì˜í•™ê³¼",           "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê²½ì˜í•™ê³¼"),
    ("ê´€ê´‘ê²½ì˜í•™ê³¼",       "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê´€ê´‘ê²½ì˜í•™ê³¼"),
    ("êµ­ì œë¬´ì—­í•™ê³¼",       "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/êµ­ì œë¬´ì—­í•™ê³¼"),
    ("ê²½ì˜ì •ë³´í•™ê³¼",       "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê²½ì˜ì •ë³´í•™ê³¼"),
    ("ê¸ˆìœµí•™ê³¼",           "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê¸ˆìœµí•™ê³¼"),
    ("ê²½ì˜ëŒ€í•™",           "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê³µí†µ_ê²½ì˜ëŒ€í•™ì†Œê°œ"),

    # ë¹„êµê³¼ & ì·¨ì—…
    ("ë¹„êµê³¼",             "ë¹„êµê³¼_DECOì‹œìŠ¤í…œ"),
    ("DECO",              "ë¹„êµê³¼_DECOì‹œìŠ¤í…œ"),
    ("ì·¨ì—…",               "ì·¨ì—…ì§€ì›_ì±„ìš©ì •ë³´"),
    ("ì±„ìš©",               "ì·¨ì—…ì§€ì›_ì±„ìš©ì •ë³´"),
]

def map_query_to_categories(query: str) -> List[str]:
    """
    ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì§ˆì˜ì—ì„œ ì¹´í…Œê³ ë¦¬ í›„ë³´ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.
    - 'í´ë”:ì¹´í…Œê³ ë¦¬/í•˜ìœ„' í˜•íƒœê°€ ìžˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì‚¬ìš©ìž ê°•ì œ ì§€ì •)
    - ê·œì¹™ê³¼ ë§¤ì¹­ë˜ëŠ” í•­ëª©ì„ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
    """
    query = (query or "").strip()
    cats: List[str] = []

    # ì‚¬ìš©ìžê°€ ì§ì ‘ ì§€ì •: ì˜ˆ) í´ë”: í•™ì‚¬ì•ˆë‚´/ì„±ì   ë˜ëŠ”  folder: ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´/ê²½ì˜í•™ê³¼
    for token in ["í´ë”:", "folder:"]:
        if token in query:
            after = query.split(token, 1)[1].strip()
            # ê³µë°±/ë¬¸ìž¥ë¶€í˜¸ ì•žê¹Œì§€ë§Œ
            after = after.split()[0].strip().strip(" ,.;)ã€ã€]")
            # ìƒëŒ€ê²½ë¡œë¥¼ í‘œì¤€í™”
            if not after.startswith(("ëŒ€í•™_ê³µí†µì •ë³´", "ê²½ì˜ëŒ€í•™_í•™ê³¼ë³„ì •ë³´", "ì·¨ì—…ì§€ì›_ì±„ìš©ì •ë³´", "ë¹„êµê³¼_DECOì‹œìŠ¤í…œ")):
                # ì‚¬ìš©ìžê°€ 'í•™ì‚¬ì•ˆë‚´/ì„±ì ' ê°™ì€ ìƒëŒ€ë§Œ ì ì—ˆì„ ìˆ˜ ìžˆìŒ â†’ ëŒ€í•™_ê³µí†µì •ë³´/ ì ‘ë‘ì–´ ë³´ì •
                after = f"ëŒ€í•™_ê³µí†µì •ë³´/{after}".replace("//", "/")
            cats.append(after.replace("\\", "/"))

    q = query.replace(" ", "")
    for key, cat in CATEGORY_RULES:
        if key in q:
            cats.append(cat)

    # ì¤‘ë³µ ì œê±° & ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ (ë„ˆë¬´ ë„“ì–´ì§€ëŠ” ê±¸ ë°©ì§€)
    uniq = []
    for c in cats:
        if c not in uniq:
            uniq.append(c)
    return uniq[:3]


# ==============================
# ðŸ”Ž 6. ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ê°€ëŠ¥í•œ Retriever
# ==============================
def make_retriever(vs: Chroma, top_k: int = TOP_K, categories: Optional[List[str]] = None):
    """
    categoriesê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬(ë“¤)ë§Œ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    search_kwargs = {"k": top_k}
    if categories:
        # Chroma í•„í„° ë¬¸ë²•: {"category": {"$in": [...]}}  ë˜ëŠ” ì •í™•ì¼ì¹˜ {"category": "..."}
        if len(categories) == 1:
            search_kwargs["filter"] = {"category": categories[0]}
        else:
            search_kwargs["filter"] = {"category": {"$in": categories}}
    return vs.as_retriever(search_kwargs=search_kwargs)


# ==============================
# ðŸ§µ 7. ëŒ€í™”í˜• RAG ì²´ì¸
# ==============================
def build_chain(vs: Chroma, categories: Optional[List[str]] = None):
    retriever = make_retriever(vs, TOP_K, categories)
    llm = ChatUpstage(model="solar-pro")

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "ë‹¹ì‹ ì€ ë™ì•„ëŒ€í•™êµ ê´€ë ¨ ê³µì‹ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” RAG ì±—ë´‡ìž…ë‹ˆë‹¤.\n"
            "ì•„ëž˜ì˜ ì°¸ê³  ë¬¸ì„œ ì¡°ê°ì„ ê·¼ê±°ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n\n"
            "ì°¸ê³  ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
            "ì§ˆë¬¸: {question}\n\n"
            "ì‘ë‹µ ê·œì¹™:\n"
            "- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
            "- í•µì‹¬ë§Œ ì •ë¦¬í•˜ì—¬ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
            "- í•„ìš”í•˜ë©´ í•­ëª©(â€¢)ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”."
        ),
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={
            "prompt": prompt,
            "document_variable_name": "context",
            "output_key": "answer",
        },
        output_key="answer",
    )
    return chain


# ==============================
# ðŸš€ 8. ë©”ì¸ ë£¨í”„
# ==============================
def main():
    print("ðŸ“š CampusFinder RAG ì´ˆê¸°í™” ì¤‘â€¦\n")
    vectorstore = build_or_load_vectorstore(ROOT_DATA_DIR, DB_PATH)
    print("\nâœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: exit/quit/ì¢…ë£Œ)\n")

    # ì¹´í…Œê³ ë¦¬ ëª¨ë“œ ì•ˆë‚´
    print("ðŸ’¡ ížŒíŠ¸) íŠ¹ì • í´ë”ë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ ì§ˆì˜ì— 'í´ë”: í•™ì‚¬ì•ˆë‚´/ì„±ì ' ì²˜ëŸ¼ ì ì–´ì£¼ì„¸ìš”.")
    print("   ì˜ˆ) 'ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜ í´ë”: í•™ì‚¬ì•ˆë‚´/ì¡¸ì—…'")

    qa_chain = build_chain(vectorstore)  # ìµœì´ˆì—” ì „ì²´ ê²€ìƒ‰

    while True:
        try:
            query = input("\nâ“ ì§ˆë¬¸: ").strip()
        except EOFError:
            break

        if query.lower() in {"exit", "quit", "ì¢…ë£Œ"}:
            print("ðŸ‘‹ í”„ë¡œê·¸ëž¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ì§ˆì˜ â†’ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
        cats = map_query_to_categories(query)
        if cats:
            print(f"ðŸ”Ž ì¹´í…Œê³ ë¦¬ ìš°ì„  ê²€ìƒ‰ ì ìš©: {cats}")
            qa_chain = build_chain(vectorstore, categories=cats)
        else:
            qa_chain = build_chain(vectorstore, categories=None)

        try:
            result = qa_chain.invoke({"question": query})
            answer = result.get("answer", "").strip()
            print(f"\nðŸ¤– ë‹µë³€:\n{answer}\n")

            # ì°¸ê³  ë¬¸ì„œ ì¶œë ¥
            if "source_documents" in result:
                seen = set()
                refs = []
                for d in result["source_documents"]:
                    meta = d.metadata or {}
                    ref = (meta.get("file_path") or meta.get("source") or "unknown", meta.get("category", ""))
                    if ref not in seen:
                        seen.add(ref)
                        refs.append(ref)

                if refs:
                    print("ðŸ“Ž ì°¸ê³  ë¬¸ì„œ:")
                    for fp, cat in refs:
                        cat_disp = f" ({cat})" if cat else ""
                        print(f" - {fp}{cat_disp}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    # ì˜µì…˜: ë£¨íŠ¸ í´ë”/DB ê²½ë¡œë¥¼ ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì „ë‹¬ ê°€ëŠ¥
    # ì‚¬ìš©: python campusfinder_rag_pipeline.py [DATA_DIR] [DB_PATH]
    if len(sys.argv) >= 2:
        ROOT_DATA_DIR = sys.argv[1]
    if len(sys.argv) >= 3:
        DB_PATH = sys.argv[2]
    main()
