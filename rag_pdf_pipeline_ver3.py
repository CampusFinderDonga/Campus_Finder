import os
from dotenv import load_dotenv
from langchain_upstage import UpstageEmbeddings, ChatUpstage, UpstageDocumentParseLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate


# ==============================
# ğŸ”§ 1. í™˜ê²½ ì„¤ì •
# ==============================
load_dotenv()
api_key = os.getenv("UPSTAGE_API_KEY")
if not api_key:
    raise ValueError("âŒ Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

PDF_FOLDER = "PDFs"
TXT_FOLDER = "PDFs_text"
DB_PATH = "chroma_db_v2"

os.makedirs(TXT_FOLDER, exist_ok=True)


# ==============================
# ğŸ“„ 2. PDF â†’ TXT ë³€í™˜
# ==============================
def convert_pdfs_to_text():
    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith(".pdf")]

    if not pdf_files:
        print("âš ï¸ PDFs í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for pdf_file in pdf_files:
        pdf_path = os.path.join(PDF_FOLDER, pdf_file)
        txt_name = pdf_file.replace(".pdf", ".txt")
        txt_path = os.path.join(TXT_FOLDER, txt_name)

        if os.path.exists(txt_path):
            print(f"ğŸ“˜ ì´ë¯¸ ë³€í™˜ë¨: {pdf_file}")
            continue

        print(f"ğŸ” PDF ë³€í™˜ ì¤‘: {pdf_file}")
        try:
            loader = UpstageDocumentParseLoader(pdf_path, split="page")
            docs = loader.load()
            texts = [doc.page_content for doc in docs]
            full_text = "\n".join(texts)
            with open(txt_path, "w", encoding="utf-8") as f:
                f.write(full_text)
            print(f"âœ… ë³€í™˜ ì™„ë£Œ: {txt_name}")
        except Exception as e:
            print(f"âŒ ë³€í™˜ ì‹¤íŒ¨ ({pdf_file}): {e}")


# ==============================
# ğŸ“˜ 3. í…ìŠ¤íŠ¸ ë¡œë“œ
# ==============================
def load_documents():
    docs = []
    for filename in os.listdir(TXT_FOLDER):
        if filename.endswith(".txt"):
            with open(os.path.join(TXT_FOLDER, filename), "r", encoding="utf-8") as f:
                text = f.read().strip()
                docs.append({"source": filename.replace(".txt", ""), "text": text})
    if not docs:
        raise ValueError("âš ï¸ PDFs_text í´ë”ì— .txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    return docs


# ==============================
# âœ‚ï¸ 4. í…ìŠ¤íŠ¸ ë¶„í• 
# ==============================
def split_documents(docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=120)
    chunks = []
    for doc in docs:
        for chunk in splitter.split_text(doc["text"]):
            chunks.append({"text": chunk, "source": doc["source"]})
    return chunks


# ==============================
# ğŸ§  5. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
# ==============================
def build_vectorstore(chunks):
    print("\nğŸ§  ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
    embedding = UpstageEmbeddings(model="solar-embedding-1-large")
    texts = [c["text"] for c in chunks]
    metadatas = [{"source": c["source"]} for c in chunks]

    vectorstore = Chroma.from_texts(
        texts=texts,
        embedding=embedding,
        metadatas=metadatas,
        persist_directory=DB_PATH
    )
    print(f"âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ! ({len(chunks)}ê°œì˜ ì²­í¬ ì €ì¥ë¨)")
    return vectorstore


# ==============================
# ğŸ’¬ 6. RAG ëŒ€í™” ì²´ì¸
# ==============================
def create_chain(vectorstore):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
    llm = ChatUpstage(model="solar-pro")

    # âœ… PromptTemplate ê°ì²´
    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "ë‹¹ì‹ ì€ ë™ì•„ëŒ€í•™êµ í•™ì‚¬ì•ˆë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” RAG ì±—ë´‡ì…ë‹ˆë‹¤.\n"
            "ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\n"
            "ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
            "ì§ˆë¬¸: {question}\n\n"
            "ê·œì¹™:\n"
            "- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ë¡ í•˜ì§€ ë§ê³  'í•´ë‹¹ ë‚´ìš©ì€ ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
            "- ë¬¸ì„œì— ê·¼ê±°í•œ ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."
        )
    )

    # âœ… memoryì—ë„ output_key ì§€ì •
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"  # âš¡ ì¶”ê°€ë¨ (í•µì‹¬ ìˆ˜ì •)
    )

    # âœ… chain êµ¬ì„±
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={
            "prompt": prompt_template,
            "document_variable_name": "context",
            "output_key": "answer"
        },
        output_key="answer"  # ì—¬ê¸°ë„ ìœ ì§€
    )
    return chain


# ==============================
# ğŸš€ 7. ë©”ì¸ ì‹¤í–‰
# ==============================
def main():
    print("ğŸ“š ë™ì•„ëŒ€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n")

    convert_pdfs_to_text()
    docs = load_documents()
    chunks = split_documents(docs)
    vectorstore = build_vectorstore(chunks)
    qa_chain = create_chain(vectorstore)

    print("\nâœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ exit)\n")

    while True:
        query = input("â“ ì§ˆë¬¸: ").strip()
        if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        try:
            result = qa_chain.invoke({"question": query})
            answer = result.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
            print(f"\nğŸ¤– ë‹µë³€:\n{answer}\n")

            if "source_documents" in result:
                print("ğŸ“ ì°¸ê³  ë¬¸ì„œ:")
                unique_sources = sorted(set([doc.metadata.get("source", "unknown") for doc in result["source_documents"]]))
                for src in unique_sources:
                    print(f" - {src}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
